{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = os.path.expanduser(\"~/.sdkman/candidates/java/17.0.17-tem\")\n",
    "os.environ[\"PATH\"] = os.path.join(os.environ[\"JAVA_HOME\"], \"bin\") + os.pathsep + os.environ[\"PATH\"]\n",
    "subprocess.run([\"java\",\"-version\"], check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203963d-fe11-4702-8be5-3b73523166a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer(as_frame=True)\n",
    "data = breast_cancer.frame\n",
    "\n",
    "# Make column names ML friendly\n",
    "data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = dict(enumerate(breast_cancer.target_names))\n",
    "class_labels = data['target'].map(target_names)\n",
    "class_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in data.columns if col != 'target']\n",
    "feature_cols[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[feature_cols].describe().T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47920a9",
   "metadata": {},
   "source": [
    "The breast cancer dataset has more benign cases than malignant ones.\n",
    "\n",
    "We will treat `target` as the binary label where 0 = malignant and 1 = benign.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target column is already binary, so no additional label engineering is required.\n",
    "sorted(data.target.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_cols = feature_cols[:12]\n",
    "dims = (3, 4)\n",
    "\n",
    "fig, axes = plt.subplots(dims[0], dims[1], figsize=(25, 15))\n",
    "for i, col in enumerate(selected_cols):\n",
    "  r, c = divmod(i, dims[1])\n",
    "  sns.boxplot(x=class_labels, y=data[col], ax=axes[r, c])\n",
    "  axes[r, c].set_title(col.replace('_', ' '))\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(selected_cols), dims[0] * dims[1]):\n",
    "  r, c = divmod(j, dims[1])\n",
    "  axes[r, c].axis('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7815f5f",
   "metadata": {},
   "source": [
    "Several of the mean-based features (for example mean radius and mean perimeter) show clear separation between malignant and benign tumors, indicating they should be useful predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ebc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X = data[feature_cols]\n",
    "y = data.target\n",
    " \n",
    "# Split out the training data\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state=123)\n",
    " \n",
    "# Split the remaining data equally into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a731588",
   "metadata": {},
   "source": [
    "### Build a baseline model\n",
    "\n",
    "This task seems well suited to a random forest classifier, since the output is binary and there may be interactions between multiple variables.\n",
    "\n",
    "The following code builds a simple classifier using scikit-learn. It uses MLflow to keep track of the model accuracy, and to save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c76d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "import cloudpickle\n",
    "import time\n",
    " \n",
    "# The predict method of sklearn's RandomForestClassifier returns a binary classification (0 or 1). \n",
    "# The following code creates a wrapper function, SklearnModelWrapper, that uses \n",
    "# the predict_proba method to return the probability that the observation belongs to each class. \n",
    " \n",
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def predict(self, context, model_input):\n",
    "    return self.model.predict_proba(model_input)[:,1]\n",
    "\n",
    "\n",
    "# mlflow.start_run creates a new MLflow run to track the performance of this model. \n",
    "# Within the context, you call mlflow.log_param to keep track of the parameters used, and\n",
    "# mlflow.log_metric to record metrics like accuracy.\n",
    "with mlflow.start_run(run_name='untuned_random_forest'):\n",
    "    n_estimators = 10\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, random_state=np.random.RandomState(123))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\n",
    "    predictions_test = model.predict_proba(X_test)[:,1]\n",
    "    auc_score = roc_auc_score(y_test, predictions_test)\n",
    "    mlflow.log_param('n_estimators', n_estimators)\n",
    "    # Use the area under the ROC curve as a metric.\n",
    "    mlflow.log_metric('auc', auc_score)\n",
    "    wrappedModel = SklearnModelWrapper(model)\n",
    "    # Log the model with a signature that defines the schema of the model's inputs and outputs. \n",
    "    # When the model is deployed, this signature will be used to validate inputs.\n",
    "    signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n",
    "    \n",
    "    # MLflow contains utilities to create a conda environment used to serve models.\n",
    "    # The necessary dependencies are added to a conda.yaml file which is logged along with the model.\n",
    "    conda_env =  _mlflow_conda_env(\n",
    "            additional_conda_deps=None,\n",
    "            additional_pip_deps=[\"cloudpickle=={}\".format(cloudpickle.__version__), \"scikit-learn=={}\".format(sklearn.__version__)],\n",
    "            additional_conda_channels=None,\n",
    "        )\n",
    "    mlflow.pyfunc.log_model(\"random_forest_model\",\n",
    "                            python_model=wrappedModel,\n",
    "                            conda_env=conda_env,\n",
    "                            signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns.tolist(), columns=['importance'])\n",
    "feature_importances.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436ff02",
   "metadata": {},
   "source": [
    "The exploratory plots above highlight that the malignant class typically has larger values for metrics like mean radius while the benign class tends to have lower values.\n",
    "\n",
    "During model training, MLflow logs the Area Under the ROC Curve (AUC). Open the Experiment Runs sidebar to inspect the logged metrics for each run.\n",
    "\n",
    "Register the model in MLflow Model Registry\n",
    "By registering this model in Model Registry, you can easily reference the model from anywhere within Databricks.\n",
    "\n",
    "The following section shows how to do this programmatically, but you can also register a model using the UI. See \"Create or register a model using the UI\" (AWS|Azure|GCP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = mlflow.search_runs(filter_string='tags.mlflow.runName = \"untuned_random_forest\"').iloc[0].run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b06c9-9b68-4887-a0fe-2959e879c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see the error \"PERMISSION_DENIED: User does not have any permission level assigned to the registered model\", \n",
    "# the cause may be that a model already exists with the name \"breast_cancer_classifier\". Try using a different name.\n",
    "model_name = \"breast_cancer_classifier\"\n",
    "model_version = mlflow.register_model(f\"runs:/{run_id}/random_forest_model\", model_name)\n",
    " \n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MlflowClient class allows you to interact with the MLflow Tracking Server programmatically. \n",
    "You can use it to perform various tasks, such as creating and managing experiments, starting \n",
    "and managing runs, logging metrics and parameters, and querying information about experiments and runs.\n",
    "\n",
    "\"\"\"\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=model_version.version,\n",
    "  stage=\"Production\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bf9b3",
   "metadata": {},
   "source": [
    "The Models page now shows the model version in stage \"Production\".\n",
    "\n",
    "You can now refer to the model using the path \"models:/breast_cancer_classifier/production\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\n",
    " \n",
    "# Sanity-check: This should match the AUC logged by MLflow\n",
    "print(f'AUC: {roc_auc_score(y_test, model.predict(X_test))}')\n",
    "AUC: 0.8540300975814177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlflow ui --port=5001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dee7da",
   "metadata": {},
   "source": [
    "## Experiment with a new model\n",
    "\n",
    "The random forest model performed well even without hyperparameter tuning.\n",
    "\n",
    "The following code uses the xgboost library to train a more accurate model. It runs a parallel hyperparameter sweep to train multiple models in parallel, using Hyperopt and SparkTrials. As before, the code tracks the performance of each parameter configuration with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt library imports for defining and executing hyperparameter optimization\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n",
    "from hyperopt.pyll import scope  \n",
    "from math import exp  \n",
    "import mlflow.xgboost\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# search_space dictionary defines the range and distribution of hyperparameters for the model\n",
    "search_space = {\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),  # Integer range for tree depth\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),  # Log-uniform distribution for learning rate\n",
    "  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),  # Log-uniform for L1 regularization term\n",
    "  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),  # Log-uniform for L2 regularization term\n",
    "  'min_child_weight': hp.loguniform('min_child_weight', -1, 3),  # Log-uniform for minimum sum of instance weight(hessian) needed in a child\n",
    "  'objective': 'binary:logistic',  # Objective function for binary classification\n",
    "  'seed': 123,  # Set a seed for deterministic training\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9b3be",
   "metadata": {},
   "source": [
    "\n",
    "Nested Runs: When you use mlflow.start_run(nested=True) within the main run, you create nested runs. These runs are associated with the main run and inherit some of its parameters and context. Nested runs are typically used to explore variations or sub-experiments within the main experiment.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Your main experiment code goes here\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Nested experiment code goes here\n",
    "Parameters and metrics logged in the nested run are associated with that specific run and can be accessed separately from the main run.\n",
    "You can create multiple nested runs within a main run to represent different variations or configurations of your experiment.\n",
    "python\n",
    "Copy code\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Your main experiment code goes here\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Nested experiment code 1 goes here\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Nested experiment code 2 goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params):\n",
    "      # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n",
    "    mlflow.xgboost.autolog()\n",
    "    with mlflow.start_run(nested=True):\n",
    "        train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "        validation = xgb.DMatrix(data=X_val, label=y_val)\n",
    "        # Pass in the validation set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric\n",
    "        # is no longer improving.\n",
    "        booster = xgb.train(params=params, dtrain=train, num_boost_round=1000,\\\n",
    "                            evals=[(validation, \"validation\")], early_stopping_rounds=50)\n",
    "        validation_predictions = booster.predict(validation)\n",
    "        auc_score = roc_auc_score(y_val, validation_predictions)\n",
    "        mlflow.log_metric('auc', auc_score)\n",
    "\n",
    "        signature = infer_signature(X_train, booster.predict(train))\n",
    "        mlflow.xgboost.log_model(booster, \"model\", signature=signature)\n",
    "\n",
    "        # Set the loss to -1*auc_score so fmin maximizes the auc_score\n",
    "        return {'status': STATUS_OK, 'loss': -1*auc_score, 'booster': booster.attributes()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pyspark, the Python API for Spark which lets you write Spark applications using Python\n",
    "from pyspark import SparkContext, SparkConf  \n",
    "\n",
    "# Setting up the configuration for Spark:\n",
    "conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\")  # Configures the Spark driver host address to localhost\n",
    "\n",
    "# Creating a SparkContext using the above configuration, essential for connecting to a Spark cluster:\n",
    "sc = SparkContext(conf=conf_spark)  # Initializes the main entry point for Spark functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75537b0e",
   "metadata": {},
   "source": [
    "TPE stands for Tree-structured Parzen Estimator. It's a Bayesian optimization technique that models the probability distribution of the hyperparameters given the observed metrics. It is particularly effective for high-dimensional spaces and has become a popular choice in machine learning for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f6f33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Greater parallelism will lead to speedups, but a less optimal hyperparameter sweep. \n",
    "# A reasonable value for parallelism is the square root of max_evals.\n",
    "spark_trials = SparkTrials(parallelism=10)\n",
    "\n",
    "# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent\n",
    "# run called \"xgboost_models\" .\n",
    "with mlflow.start_run(run_name='xgboost_models'):\n",
    "  best_params = fmin(\n",
    "    fn=train_model, \n",
    "    space=search_space, \n",
    "    algo=tpe.suggest,\n",
    "    max_evals=96,\n",
    "    trials=spark_trials,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c706e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = mlflow.search_runs(order_by=['metrics.auc DESC']).iloc[0]\n",
    "print(f'AUC of Best Run: {best_run[\"metrics.auc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f81372",
   "metadata": {},
   "source": [
    "## Update the production breast_cancer_classifier model in MLflow Model Registry\n",
    "\n",
    "Earlier, you saved the baseline model to Model Registry with the name `breast_cancer_classifier`. Now that you have created a more accurate model, update `breast_cancer_classifier`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version = mlflow.register_model(f\"runs:/{best_run.run_id}/model\", model_name)\n",
    "\n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a4239",
   "metadata": {},
   "source": [
    "Click Models in the left sidebar to see that the `breast_cancer_classifier` model now has two versions.\n",
    "\n",
    "The following code promotes the new version to production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba884129",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage='Archived'\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=new_model_version.version,\n",
    "    stage='Production'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0a887",
   "metadata": {},
   "source": [
    "Clients that call load_model now receive the new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31991db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, model.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d7d35",
   "metadata": {},
   "source": [
    "## Batch inference\n",
    "\n",
    "There are many scenarios where you might want to evaluate a model on a corpus of new data. For example, you may have a fresh batch of data, or may need to compare the performance of two models on the same corpus of data.\n",
    "\n",
    "The following code evaluates the model on data stored in a Delta table, using Spark to run the computation in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d0e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function mlflow.pyfunc.spark_udf(spark, model_uri, result_type=None, env_manager=None, params: dict[str, typing.Any] | None = None, extra_env: dict[str, str] | None = None, prebuilt_env_uri: str | None = None, model_config: str | pathlib.Path | dict[str, typing.Any] | None = None)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.pyfunc.spark_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a972228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 2401.09it/s] \n",
      "2025/11/03 21:31:46 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n",
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 3121.92it/s] \n",
      "2025/11/03 21:31:46 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or retrieve a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLflow Integration\") \\\n",
    "    .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "apply_model_udf = mlflow.pyfunc.spark_udf(spark, f\"models:/{model_name}/production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1f3ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = spark.read.format(\"csv\").load(table_path) # table_path is path to the delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0c1b95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1762222824419, current_stage='None', deployment_job_state=None, description=None, last_updated_timestamp=1762222824419, metrics=[<Metric: dataset_digest=None, dataset_name=None, key='validation-logloss', model_id='m-d4913e98fefa467ea505dd56a5e0cfe8', run_id='3614da3b817d4cddbbd0e5c8a15c9f3b', step=0, timestamp=1762222783851, value=0.3957874968386533>,\n",
       " <Metric: dataset_digest='bf39842e', dataset_name='dataset', key='validation-logloss', model_id='m-d4913e98fefa467ea505dd56a5e0cfe8', run_id='3614da3b817d4cddbbd0e5c8a15c9f3b', step=0, timestamp=1762222788677, value=0.05244956117096815>,\n",
       " <Metric: dataset_digest='bf39842e', dataset_name='dataset', key='stopped_iteration', model_id='m-d4913e98fefa467ea505dd56a5e0cfe8', run_id='3614da3b817d4cddbbd0e5c8a15c9f3b', step=0, timestamp=1762222788677, value=78.0>,\n",
       " <Metric: dataset_digest=None, dataset_name=None, key='auc', model_id='m-d4913e98fefa467ea505dd56a5e0cfe8', run_id='3614da3b817d4cddbbd0e5c8a15c9f3b', step=0, timestamp=1762222789197, value=1.0>,\n",
       " <Metric: dataset_digest='bf39842e', dataset_name='dataset', key='best_iteration', model_id='m-d4913e98fefa467ea505dd56a5e0cfe8', run_id='3614da3b817d4cddbbd0e5c8a15c9f3b', step=0, timestamp=1762222788677, value=29.0>], model_id='m-d4913e98fefa467ea505dd56a5e0cfe8', name='breast_cancer_classifier', params={'custom_metric': 'None',\n",
       " 'early_stopping_rounds': '50',\n",
       " 'learning_rate': '0.4192118565705786',\n",
       " 'max_depth': '25',\n",
       " 'maximize': 'None',\n",
       " 'min_child_weight': '2.9215748238887187',\n",
       " 'num_boost_round': '1000',\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': '0.00927962401507643',\n",
       " 'reg_lambda': '0.0035457968585411773',\n",
       " 'seed': '123',\n",
       " 'verbose_eval': 'True'}, run_id='3614da3b817d4cddbbd0e5c8a15c9f3b', run_link=None, source='models:/m-d4913e98fefa467ea505dd56a5e0cfe8', status='READY', status_message=None, tags={}, user_id=None, version=12>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07e1b6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3614da3b817d4cddbbd0e5c8a15c9f3b'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_version.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve the model using the MLflow Model Serving\n",
    "\n",
    "# Run the following in terminal outiside jupyter and after activating the virtual environment\n",
    "# mlflow models serve --env-manager=local -m models:/breast_cancer_classifier/production -h 0.0.0.0 -p 5001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e4814",
   "metadata": {},
   "source": [
    "- Here **model_name** is `breast_cancer_classifier`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0.977245032787323, 0.0018812912749126554, 0.0004756989947054535, 0.08485926687717438, 0.9392794370651245, 0.9962750673294067, 0.002048053778707981, 0.03223269805312157, 0.9994556307792664, 0.0013605729909613729, 0.005775751080363989, 0.9937363862991333, 0.9992714524269104, 0.0009298113873228431, 0.9968335032463074, 0.9934667944908142, 0.9963095784187317, 0.9948203563690186, 0.9991143345832825, 0.01945744827389717, 0.0036211004480719566, 0.9987717270851135, 0.004347221460193396, 0.002124169608578086, 0.9989701509475708, 0.2190239578485489, 0.9990696310997009, 0.9985594153404236, 0.001434369245544076, 0.9997492432594299, 0.999270498752594, 0.995508074760437, 0.996904194355011, 0.9988119602203369, 0.9850181937217712, 0.0034078070893883705, 0.9961591958999634, 0.33427298069000244, 0.016329968348145485, 0.0011782258516177535, 0.01714150235056877, 0.0007764332112856209, 0.9968259334564209, 0.00026044456171803176, 0.9909340143203735, 0.9950013756752014, 0.9859433174133301, 0.9966626763343811, 0.9971346855163574, 0.001324556884355843, 0.0005685656215064228, 0.0033007243182510138, 0.0016137410420924425, 0.9935488104820251, 0.9964333772659302, 0.9983876943588257, 0.9989626407623291, 0.9804203510284424, 0.9996116757392883, 0.9991627931594849, 0.9990515112876892, 0.004525909200310707, 0.9944658875465393, 0.9919301867485046, 0.001434369245544076, 0.002584597794339061, 0.997958779335022, 0.9972442388534546, 0.994450569152832, 0.9894128441810608, 0.9979488253593445, 0.0017174939857795835, 0.1921941190958023, 0.9987745881080627, 0.9981434345245361, 0.9989405274391174, 0.9992223978042603, 0.013212142512202263, 0.033167045563459396, 0.9875297546386719, 0.9968335032463074, 0.9950122237205505, 0.9487829208374023, 0.002472224412485957, 0.9748143553733826, 0.9995904564857483, 0.97517991065979, 0.9136258959770203, 0.0007528415299020708, 0.9988093376159668, 0.0017403472447767854, 0.9993554949760437, 0.9991759657859802, 0.0007528415299020708, 0.001434369245544076, 0.972588837146759, 0.002869429299607873, 0.00039719967753626406, 0.9989394545555115, 0.10184868425130844, 0.001222240854986012, 0.001434369245544076, 0.987993597984314, 0.6705970168113708, 0.0018851847853511572, 0.9964583516120911, 0.009503551758825779, 0.9989504218101501, 0.9964613318443298, 0.9988093376159668, 0.9982424974441528, 0.9671341180801392, 0.9755182266235352, 0.015394766815006733]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:5002/invocations'\n",
    "\n",
    "datads_dict = {\"dataframe_split\": X_test.to_dict(orient='split')}\n",
    "\n",
    "response = requests.post(url, json=datads_dict)\n",
    "predictions = response.json()\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072da51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
